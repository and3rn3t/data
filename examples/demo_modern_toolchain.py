#!/usr/bin/env python3
"""
Modern Data Science Toolchain Demo

This script demonstrates the complete modern data science workflow
using all the newly integrated tools in the Data Science Sandbox.

Run with: python demo_modern_toolchain.py
"""

import warnings

warnings.filterwarnings("ignore")

print("üöÄ MODERN DATA SCIENCE TOOLCHAIN DEMONSTRATION")
print("=" * 60)

try:
    # Import our integration modules
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score
    from sklearn.model_selection import cross_val_score, train_test_split

    from sandbox.integrations import (
        DataPipelineBuilder,
        ExperimentTracker,
        HyperparameterOptimizer,
        ModelExplainer,
        ModernDataProcessor,
    )

    print("‚úÖ All modules imported successfully!")

except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("üí° Run: pip install -e .[all] to install all dependencies")
    exit(1)


def demo_data_processing():
    """Demonstrate high-performance data processing."""
    print("\nüìä STEP 1: High-Performance Data Processing")
    print("-" * 50)

    # Initialize modern data processor
    processor = ModernDataProcessor()

    # Show available tools
    print("üîç Checking available tools:")
    status = processor.performance_comparison_demo()

    for tool, info in status.items():
        if isinstance(info, str):
            print(f"  ‚Ä¢ {tool.replace('_', ' ').title()}: {info}")

    return processor


def demo_ml_tracking():
    """Demonstrate ML experiment tracking."""
    print("\nüß™ STEP 2: ML Experiment Tracking")
    print("-" * 50)

    # Initialize experiment tracker
    tracker = ExperimentTracker()

    # Show tracking capabilities
    print("üìã Experiment tracking capabilities:")
    tracking_info = tracker.get_tracking_info()

    for capability, status in tracking_info.items():
        print(f"  ‚Ä¢ {capability.replace('_', ' ').title()}: {status}")

    return tracker


def demo_baseline_model(tracker, X_train, X_test, y_train, y_test):
    """Train and evaluate baseline model."""
    print("\nüå≤ Training baseline Random Forest...")

    # Start experiment run
    tracker.start_run(
        run_name="baseline_random_forest",
        tags={"model_type": "random_forest", "dataset": "sales", "demo": True},
    )

    # Train baseline model
    rf_model = RandomForestClassifier(
        n_estimators=100, random_state=42, min_samples_leaf=1, max_features="sqrt"
    )
    rf_model.fit(X_train, y_train)

    # Evaluate model
    pred = rf_model.predict(X_test)
    accuracy = accuracy_score(y_test, pred)
    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5)

    return rf_model, accuracy, cv_scores


def demo_hyperparameter_optimization(X_train, y_train):
    """Demonstrate hyperparameter optimization."""
    print("\nüéØ STEP 3: Hyperparameter Optimization")
    print("-" * 50)

    optimizer = HyperparameterOptimizer()
    opt_tools = optimizer.get_tool_comparison()
    print("üéØ Optimization tools:")
    for tool, info in opt_tools.items():
        print(f"  ‚Ä¢ {tool.upper()}: {info['status']}")

    # Define optimization objective
    def rf_objective(params):
        """Objective function for Random Forest optimization."""
        model = RandomForestClassifier(
            n_estimators=params["n_estimators"],
            max_depth=params["max_depth"] if params["max_depth"] > 0 else None,
            min_samples_split=params["min_samples_split"],
            min_samples_leaf=1,
            max_features="sqrt",
            random_state=42,
        )

        # Use cross-validation for robust estimation
        cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring="accuracy")
        return -cv_scores.mean()  # Negative because we minimize

    # Define parameter space
    param_space = {
        "n_estimators": {"type": "int", "low": 50, "high": 200},
        "max_depth": {"type": "int", "low": 1, "high": 20},
        "min_samples_split": {"type": "int", "low": 2, "high": 20},
    }

    # Run optimization
    print("‚ö° Running hyperparameter optimization...")
    optimization_results = optimizer.optimize(rf_objective, param_space, n_trials=25)

    return optimization_results


def main():
    """Main demonstration workflow."""
    # Demo data processing
    processor = demo_data_processing()

    # Create sample dataset using the fastest available method
    print("\nüè≠ Creating sample dataset (50K records)...")
    sales_data = processor.create_sample_dataset(n_rows=50000, dataset_type="sales")

    print(
        f"‚úÖ Dataset created: {type(sales_data)} with {getattr(sales_data, 'shape', 'unknown shape')}"
    )

    # Demonstrate SQL queries on DataFrame
    print("\nüóÉÔ∏è SQL Analysis on DataFrame:")
    sql_query = "SELECT region, AVG(sales_amount) as avg_sales FROM df GROUP BY region ORDER BY avg_sales DESC"

    try:
        result = processor.query_with_sql(sales_data, sql_query)
        print("  Top regions by average sales:")
        if hasattr(result, "head"):
            print(result.to_string(index=False))
        else:
            print(result)
    except Exception as e:
        print(f"  SQL query failed (fallback mode): {e}")

    # ============================================================================
    # STEP 2: Data Quality & Pipeline Engineering
    # ============================================================================

    print("\nüîß STEP 2: Data Engineering & Quality")
    print("-" * 50)

    # Initialize data pipeline builder
    pipeline_builder = DataPipelineBuilder()

    # Convert to pandas for downstream processing
    if hasattr(sales_data, "to_pandas"):
        ml_data = sales_data.to_pandas()
    else:
        ml_data = sales_data.copy()

    # Generate data quality report
    print("üìã Generating data quality report...")
    quality_report = pipeline_builder.create_data_quality_report(ml_data)

    print(f"  ‚Ä¢ Data Quality Score: {quality_report['data_quality_score']:.1f}/100")
    print(
        f"  ‚Ä¢ Missing Data: {quality_report['dataset_overview']['missing_percentage']:.1f}%"
    )
    print(f"  ‚Ä¢ Duplicate Rows: {quality_report['dataset_overview']['duplicate_rows']}")

    # Create validation suite
    print("\nüõ°Ô∏è Creating data validation suite...")
    validation_suite = pipeline_builder.create_data_validation_suite(ml_data)
    print(f"  ‚Ä¢ Method: {validation_suite['method']}")
    print(f"  ‚Ä¢ Expectations: {len(validation_suite['expectations'])}")

    # Build data pipeline
    print("\n‚öôÔ∏è Building data processing pipeline...")
    pipeline_result = pipeline_builder.create_data_pipeline(
        pipeline_name="sales_data_preprocessing",
        data_source=ml_data,
        transformations=None,  # Use default transformations
    )

    print(
        f"  ‚Ä¢ Pipeline Status: {'‚úÖ Success' if pipeline_result['success'] else '‚ùå Failed'}"
    )
    print(f"  ‚Ä¢ Original Shape: {pipeline_result['original_shape']}")
    print(f"  ‚Ä¢ Final Shape: {pipeline_result['final_shape']}")
    print(f"  ‚Ä¢ Duration: {pipeline_result.get('duration_seconds', 0):.2f} seconds")

    # Use processed data
    processed_data = pipeline_result["processed_data"]

    # ============================================================================
    # STEP 3: ML Experiment Tracking Setup
    # ============================================================================

    print("\nüìä STEP 3: ML Experiment Tracking")
    print("-" * 50)

    # Initialize experiment tracker
    tracker = ExperimentTracker(
        project_name="modern-toolchain-demo", experiment_name="sales-prediction"
    )

    # Show tracking capabilities
    tracking_info = tracker.create_comparison_demo()
    print("üîç Experiment tracking capabilities:")
    for tool, info in tracking_info.items():
        if isinstance(info, dict) and "status" in info:
            print(f"  ‚Ä¢ {tool.upper()}: {info['status']}")

    # Prepare ML dataset
    print("\nüéØ Preparing machine learning dataset...")

    # Create target variable (high profit prediction)
    processed_data["high_profit"] = (
        processed_data["profit_margin"] > processed_data["profit_margin"].median()
    ).astype(int)

    # Select features
    feature_cols = ["sales_amount", "shipping_cost"]
    X = processed_data[feature_cols]
    y = processed_data["high_profit"]

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print(f"  ‚Ä¢ Training set: {X_train.shape}")
    print(f"  ‚Ä¢ Test set: {X_test.shape}")

    # ============================================================================
    # STEP 4: Model Training with Experiment Tracking
    # ============================================================================

    print("\nü§ñ STEP 4: Model Training & Tracking")
    print("-" * 50)

    # Start experiment run
    tracker.start_run(
        run_name="baseline_random_forest",
        tags={"model_type": "random_forest", "dataset": "sales", "demo": True},
    )

    # Train baseline model
    print("üå≤ Training baseline Random Forest...")
    rf_model = RandomForestClassifier(
        n_estimators=100, random_state=42, min_samples_leaf=1, max_features="sqrt"
    )
    rf_model.fit(X_train, y_train)

    # Log parameters
    tracker.log_params(
        {
            "n_estimators": 100,
            "max_depth": None,
            "min_samples_split": 2,
            "random_state": 42,
            "model_type": "RandomForest",
        }
    )

    # Evaluate model
    y_pred = rf_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5)

    # Log metrics
    tracker.log_metrics(
        {
            "accuracy": accuracy,
            "cv_mean": cv_scores.mean(),
            "cv_std": cv_scores.std(),
            "train_samples": len(X_train),
            "test_samples": len(X_test),
        }
    )

    # Log model
    tracker.log_model(rf_model, "baseline_rf", framework="sklearn")

    print(f"  ‚Ä¢ Baseline Accuracy: {accuracy:.4f}")
    print(f"  ‚Ä¢ CV Score: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

    # End experiment run
    tracker.end_run()

    # ============================================================================
    # STEP 5: Model Explainability
    # ============================================================================

    print("\nüîç STEP 5: Model Explainability")
    print("-" * 50)

    # Initialize model explainer
    explainer = ModelExplainer()

    # Show explainability tools
    explainer_tools = explainer.get_tool_comparison()
    print("üß† Model explainability tools:")
    for tool, info in explainer_tools.items():
        print(f"  ‚Ä¢ {tool.upper()}: {info['status']}")

    # Generate model explanations
    print("\nüéØ Generating model explanations...")
    explanation_results = explainer.explain_prediction(
        model=rf_model,
        X_train=X_train,
        X_explain=X_test[:10],  # Explain first 10 predictions
        method="auto",
        feature_names=feature_cols,
    )

    print(f"  ‚Ä¢ Method used: {explanation_results['method_used']}")
    print("  ‚Ä¢ Feature Importance:")
    for feature, importance in explanation_results.get(
        "feature_importance", {}
    ).items():
        print(f"    - {feature}: {importance:.4f}")

    # Create evaluation report
    print("\nüìä Creating comprehensive evaluation report...")
    evaluation_report = explainer.create_model_evaluation_report(
        model=rf_model, X_test=X_test, y_test=y_test, task_type="classification"
    )

    print(f"  ‚Ä¢ Evaluation completed with {evaluation_report['task_type']} metrics")
    print(
        f"  ‚Ä¢ Visualizations: {'‚úÖ' if evaluation_report.get('yellowbrick_plots') else 'üìä Basic'}"
    )

    # ============================================================================
    # STEP 6: Hyperparameter Optimization
    # ============================================================================

    print("\n‚ö° STEP 6: Hyperparameter Optimization")
    print("-" * 50)

    # Initialize optimizer
    optimizer = HyperparameterOptimizer()

    # Show optimization tools
    opt_tools = optimizer.get_tool_comparison()
    print("üéØ Optimization tools:")
    for tool, info in opt_tools.items():
        print(f"  ‚Ä¢ {tool.upper()}: {info['status']}")

    # Define optimization objective
    def rf_objective(params):
        """Objective function for Random Forest optimization."""
        model = RandomForestClassifier(
            n_estimators=params["n_estimators"],
            max_depth=params["max_depth"] if params["max_depth"] > 0 else None,
            min_samples_split=params["min_samples_split"],
            min_samples_leaf=1,
            max_features="sqrt",
            random_state=42,
        )

        # Use cross-validation for robust estimation
        cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring="accuracy")
        return -cv_scores.mean()  # Negative because we minimize

    # Define parameter space
    param_space = {
        "n_estimators": {"type": "int", "low": 50, "high": 200},
        "max_depth": {"type": "int", "low": 3, "high": 15},
        "min_samples_split": {"type": "int", "low": 2, "high": 10},
    }

    print("\nüöÄ Running hyperparameter optimization...")
    print("   (This may take a minute...)")

    # Run optimization (limited trials for demo)
    optimization_results = optimizer.optimize_model(
        objective_function=rf_objective,
        param_space=param_space,
        n_trials=15,  # Reduced for demo speed
        method="auto",
        study_name="rf_optimization_demo",
    )

    print(f"  ‚Ä¢ Optimization method: {optimization_results['method']}")
    print("  ‚Ä¢ Best parameters found:")
    for param, value in optimization_results["best_params"].items():
        print(f"    - {param}: {value}")
    print(f"  ‚Ä¢ Best CV score: {-optimization_results['best_value']:.4f}")
    print(f"  ‚Ä¢ Total trials: {optimization_results['n_trials']}")

    # ============================================================================
    # STEP 7: Final Model with Optimized Parameters
    # ============================================================================

    print("\nüèÜ STEP 7: Final Optimized Model")
    print("-" * 50)

    # Start new experiment for optimized model
    tracker.start_run(
        run_name="optimized_random_forest",
        tags={"model_type": "random_forest", "optimized": True, "demo": True},
    )

    # Train optimized model
    print("üåü Training optimized model...")
    optimized_params = optimization_results["best_params"].copy()
    if (
        optimized_params.get("max_depth") is not None
        and optimized_params["max_depth"] <= 0
    ):
        optimized_params["max_depth"] = None

    # Add required hyperparameters if not present
    if "min_samples_leaf" not in optimized_params:
        optimized_params["min_samples_leaf"] = 1
    if "max_features" not in optimized_params:
        optimized_params["max_features"] = "sqrt"

    optimized_rf = RandomForestClassifier(**optimized_params, random_state=42)
    optimized_rf.fit(X_train, y_train)

    # Evaluate optimized model
    optimized_pred = optimized_rf.predict(X_test)
    optimized_accuracy = accuracy_score(y_test, optimized_pred)
    optimized_cv = cross_val_score(optimized_rf, X_train, y_train, cv=5)

    improvement = optimized_accuracy - accuracy

    # Log optimized results
    tracker.log_params(optimized_params)
    tracker.log_metrics(
        {
            "accuracy": optimized_accuracy,
            "cv_mean": optimized_cv.mean(),
            "cv_std": optimized_cv.std(),
            "improvement_over_baseline": improvement,
            "optimization_trials": optimization_results["n_trials"],
        }
    )

    # Log optimized model
    tracker.log_model(optimized_rf, "optimized_rf", framework="sklearn")

    print(f"  ‚Ä¢ Optimized Accuracy: {optimized_accuracy:.4f}")
    print(f"  ‚Ä¢ CV Score: {optimized_cv.mean():.4f} ¬± {optimized_cv.std():.4f}")
    print(f"  ‚Ä¢ Improvement: {improvement:.4f} ({improvement/accuracy*100:+.1f}%)")

    tracker.end_run()

    # ============================================================================
    # STEP 8: Summary & Recommendations
    # ============================================================================

    print("\nüìã STEP 8: Demo Summary & Recommendations")
    print("-" * 50)

    # Get experiment summary
    experiment_summary = tracker.get_experiment_summary()

    print("üéâ MODERN DATA SCIENCE TOOLCHAIN DEMO COMPLETED!")
    print(
        f"‚úÖ Processed dataset: {pipeline_result['final_shape'][0]:,} rows, {pipeline_result['final_shape'][1]} columns"
    )
    print(f"‚úÖ Data quality score: {quality_report['data_quality_score']:.1f}/100")
    print(f"‚úÖ ML experiments tracked: {experiment_summary['total_runs']}")
    print(f"‚úÖ Model explanations: {explanation_results['method_used']}")
    print(
        f"‚úÖ Hyperparameter optimization: {optimization_results['method']} with {optimization_results['n_trials']} trials"
    )
    print(f"‚úÖ Performance improvement: {improvement/accuracy*100:+.1f}%")

    print("\nüéØ Key Takeaways:")
    print("1. üöÄ Modern data processing tools significantly improve performance")
    print("2. üîß Data pipelines ensure reproducible and high-quality workflows")
    print("3. üìä Experiment tracking is essential for ML project management")
    print("4. üîç Model explainability builds trust and provides insights")
    print("5. ‚ö° Automated optimization can improve model performance")
    print("6. üè≠ These tools work seamlessly together in production environments")

    print("\nüìö Next Steps:")
    print(
        "‚Ä¢ Try the Level 7 challenge: challenges/level_7/challenge_1_modern_toolchain.md"
    )
    print("‚Ä¢ Explore individual tools with larger datasets")
    print("‚Ä¢ Set up MLflow UI for experiment visualization: mlflow ui")
    print("‚Ä¢ Experiment with different optimization algorithms")
    print("‚Ä¢ Build your own data science projects using these tools")

    print("\nüèÖ Congratulations! You've experienced the modern data science toolchain.")
    print("You're now ready to build production-ready ML systems! üöÄ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Demo interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Demo failed with error: {e}")
        print("üí° Make sure all dependencies are installed: pip install -e .[all]")
        import traceback

        traceback.print_exc()
