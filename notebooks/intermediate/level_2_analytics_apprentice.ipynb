{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0acc1a44",
   "metadata": {},
   "source": [
    "# ðŸ¥ˆ Level 2: Analytics Apprentice\n",
    "\n",
    "Welcome to Level 2 of the Data Science Sandbox! You've mastered the basics, now it's time to become an analytics apprentice. This level focuses on:\n",
    "\n",
    "- **Advanced Data Cleaning**: Handle messy, real-world data with confidence\n",
    "- **Statistical Analysis**: Master descriptive statistics, hypothesis testing, and correlation analysis\n",
    "- **Data Validation**: Use modern tools like Pandera for automated data quality checks\n",
    "- **Modern Data Science**: Work with cutting-edge tools and best practices\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this level, you'll be able to:\n",
    "- Clean and validate complex datasets using multiple strategies\n",
    "- Perform comprehensive statistical analysis with proper interpretation\n",
    "- Implement automated data quality checks and monitoring\n",
    "- Use modern data science tools for professional workflows\n",
    "\n",
    "## ðŸ“Š Datasets Used\n",
    "\n",
    "- **Messy Sales Data**: E-commerce data with intentional quality issues\n",
    "- **Analytics Data**: Customer behavior data for statistical analysis  \n",
    "- **Survey Data**: Customer satisfaction and segmentation data\n",
    "\n",
    "Let's begin your journey to becoming an analytics apprentice! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bed02c",
   "metadata": {},
   "source": [
    "## ðŸ“š Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need for our analytics journey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19085b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, chi2_contingency, ttest_ind, f_oneway\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Data validation and quality\n",
    "import pandera as pa\n",
    "from pandera import Column, DataFrameSchema, Check\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ðŸŽ¯ Ready to start Level 2: Analytics Apprentice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b3c5e",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Challenge 1: Advanced Data Cleaning\n",
    "\n",
    "Let's start with our first challenge - cleaning messy, real-world data. This dataset contains intentional data quality issues that you'll commonly encounter in practice.\n",
    "\n",
    "### What You'll Learn:\n",
    "- Multiple strategies for handling missing values\n",
    "- Outlier detection using statistical methods (IQR, Z-score)\n",
    "- Data validation using business rules\n",
    "- Creating comprehensive data quality reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the messy sales dataset\n",
    "try:\n",
    "    messy_sales = pd.read_csv('../data/datasets/messy_sales_data.csv')\n",
    "    print(f\"âœ… Dataset loaded: {len(messy_sales)} records\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Dataset not found. Let's create sample messy data:\")\n",
    "    # Create sample messy dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    messy_sales = pd.DataFrame({\n",
    "        'customer_id': range(1, n_samples + 1),\n",
    "        'customer_name': ['Customer ' + str(i) if i % 10 != 0 else np.nan for i in range(1, n_samples + 1)],\n",
    "        'customer_age': np.random.normal(35, 12, n_samples),\n",
    "        'email': ['customer' + str(i) + '@email.com' if i % 15 != 0 else 'invalid_email' for i in range(1, n_samples + 1)],\n",
    "        'phone': [f'+1-555-{str(i).zfill(4)}' if i % 20 != 0 else np.nan for i in range(1, n_samples + 1)],\n",
    "        'sales': np.random.gamma(2, 100, n_samples),\n",
    "        'product_category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'], n_samples),\n",
    "        'customer_satisfaction': np.random.choice([1, 2, 3, 4, 5, np.nan], n_samples, p=[0.05, 0.1, 0.2, 0.4, 0.2, 0.05]),\n",
    "        'registration_date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),\n",
    "        'is_premium': np.random.choice([True, False], n_samples, p=[0.3, 0.7])\n",
    "    })\n",
    "    \n",
    "    # Add intentional issues\n",
    "    messy_sales.loc[messy_sales.index[:10], 'sales'] = messy_sales['sales'].max() * 10  # Extreme outliers\n",
    "    messy_sales.loc[messy_sales.index[10:20], 'customer_age'] = -5  # Invalid ages\n",
    "    messy_sales.loc[messy_sales.index[20:25], 'sales'] = -100  # Negative sales\n",
    "\n",
    "print(\"ðŸ“Š Dataset Overview:\")\n",
    "print(f\"Shape: {messy_sales.shape}\")\n",
    "print(f\"Memory usage: {messy_sales.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\nðŸ” Basic Information:\")\n",
    "messy_sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56e852",
   "metadata": {},
   "source": [
    "### Step 1: Missing Values Analysis\n",
    "\n",
    "Let's analyze the missing values in our dataset and develop strategies to handle them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive missing values analysis\n",
    "missing_summary = messy_sales.isnull().sum()\n",
    "missing_percentage = (missing_summary / len(messy_sales)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_summary,\n",
    "    'Missing_Percentage': missing_percentage,\n",
    "    'Data_Type': messy_sales.dtypes\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"ðŸ“Š Missing Values Analysis:\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing values pattern\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Missing values heatmap\n",
    "sns.heatmap(messy_sales.isnull(), cbar=True, ax=ax1, cmap='viridis')\n",
    "ax1.set_title('Missing Values Heatmap')\n",
    "ax1.set_xlabel('Columns')\n",
    "ax1.set_ylabel('Rows (sample)')\n",
    "\n",
    "# Missing values bar chart\n",
    "missing_counts = messy_sales.isnull().sum()[messy_sales.isnull().sum() > 0]\n",
    "missing_counts.plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Missing Values Count by Column')\n",
    "ax2.set_ylabel('Number of Missing Values')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Insights:\")\n",
    "print(f\"â€¢ Total missing values: {missing_summary.sum()}\")\n",
    "print(f\"â€¢ Columns with missing data: {(missing_summary > 0).sum()}\")\n",
    "print(f\"â€¢ Most affected column: {missing_summary.idxmax()} ({missing_summary.max()} missing)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
